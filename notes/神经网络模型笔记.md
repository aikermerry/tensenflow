# 神经网络的搭建

1. 准备数据,提取特征
2. 搭建NN结构,从输入到输出(先搭建计算图,再用会话执行)
3. 大量特征数据喂给NN,迭代优化NN参数
4. 使用训练好的模型预测

# 前向传播

​	前行传播就是搭建模型的计算过程,让模型具有推理能力,可以针对一组输入给出相应的输出.

1. 变量初始化、计算图解点运算都要用会话实现

   ```
   with tf.Session() as sess:
   	sess.run()
   ```

2. 变量初始化,在sess.run()函数中使用tf.global_varibalizer()汇总所有待优化的变量

3. 计算图节点云钻：在sess,run()函数中写入待运算的节点

   sess.run(y)

4. 使用tf.placeholder(tf.float32,shape=(1,2))占位

5. 使用feed_dict＝(x:{[[],[],[]]})来喂数据

   ```
   sess.run(y,feed_dict={x:[[1,2],[2,4]]}
   ```

# 反向传播

1. 反向传播：训练模型参数，在所有参数上使用梯度下降，是ＮＮ模型在训练数据上损失函数最小

2. 损失函数（loss）：损失函数有很多，常用的是ＭＳＥ均方差

   ​                                        $MSE(Y\_,Y)=\frac{\sum^n_{n-1}{(y-y\_)}^2}{n}$

   表示为loss_mse = tf.reduce_mean(tf.square(y-y_))

3. 反向传播训练方法：以减小loss值为优化，有梯度下降，momentum优化器，adam优化器等优化方法．

   train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)

   train_step=tf.train.MomentumOptimizer(learning_rate).minimize(loss)

   train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)

   三者的区别:

   :yum:使用的是梯度下降算法，是参数沿着梯度的反方向，级总损失减小的方向移动实现更新参数

   :yum:第二个在参数更新是利用了超参数

   :yum:第三个是利用自适应学习率的算法的优化方法，adam算法和梯度下降算法不同，随机梯度下降算法是保持单一的学习率不变，而adam会通过计算梯度的一节矩阵估计个而阶估计而为不同的参数设计单独的自适应学习率

4. 学习率：就是算法参数每次更新的幅度，也反应损失函数的收敛的快慢
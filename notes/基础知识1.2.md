## 1.张量

```
zero_tsr=tf.zeros([row_dim,col_dim])
one_tsr = tf.ones([row,dim])
filled_tsr = tf.fill([row,dim],num)指定数值
constant_tsr = tf.constant([1,2,3])
rangdom_tsr = tf.random_normal([row,dom])正太随机集
randunif_tsr = tf.random_uniform([row,dim])随机均匀集
runcnorm_tsr = tf.truncated_normal(rom,dim)有边界值偶读正态随机集
```

声明好张量后使用`my_var = tf.Variable(tf.zeros([rom,dim]))`来是张量作为变量

创建变量后需要初始化变量

```
方法一：
sess.run(my_var.initializer)
方法二：
myinit＝tf.global_varibales_initializer()#用来初始化全部变量
```

##2. 矩阵

矩阵使用numpy数组来创建也可以通过diag（）函数从一个一维数组或列表来创建对角矩阵

## 3. 申明操作

没啥东西，翻书或者百度一下，太多了记不到

## 4.激励函数

激励函数的目的是为了调节权重和误差，激励是作用在张量上的非线性操作

1. 整流线性单元－－－－$max(0,x)$连续但不平滑、

   ```
   >>> print(sess.run(tf.nn.relu([-3.,-2.,1.,3.])))
   [0. 0. 1. 3.]
   ```

   

2. 为了解绝ReLU激励函数线性增长部分，会在$min()$函数中嵌套$max(0,x)$

```
>>> print(sess.run(tf.nn.relu6([-1.,-2.,4.,7.,8.])))
[0. 0. 4. 6. 6.]

```

3. sigmoid函数是最常用的连续、平滑的激励函数表示为$1/(1+e^{-x})$，函数由于在反向传播项趋近于０，因此不太使用，取值范围(-1,-1)

   ```
   >>> print(sess.run(tf.nn.sigmoid([-1.,-3.,3.,4.])))
   [0.26894143 0.04742587 0.95257413 0.98201376]
   
   ```

   

4. 双曲线正切函数：取值范围（０，1）,表示为$(exp(x)-exp(-x))/(exp(x)+exp(-x))$

   ```
   >>> print(sess.run(tf.nn.tanh([-1.,-3.,3.,4.])))
   [-0.7615942 -0.9950547  0.9950547  0.9993292]
   
   ```

   

   

5. 还有,softplus，softsing, 输入增加是前者趋近无穷大，后者趋近１，反之，前者趋近０，后者趋近-1

6. ELU激活函数


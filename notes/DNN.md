# DNN 算法

## 1. 梯度函数

梯度函数有梯度上升和梯度下降，求损失函数的最小值是使用梯度下降函数，反之使用梯度上升函数也可以表达为求 $-f(θ) $  的最大值。

## 2. 损失函数

损失函数是微党课评估模型的好坏，损失函数极小意味着拟合度最好，对应的模型参数也就是最优参数。

## 3. 正则化

L1&L2、Boosting&Bagging(集成学习的思路正则化)

## 4. 前向传播

$a^l=σ(Z^l=σ(W^la^{l-1}+b^l)$

![img](https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220122323148-1704308672.png)

## 5. 反向传播

​	即我们使输入层有n_in个神经元，而输出层有n_out个神经元。再加上一些含有若干神经元的隐藏层。 

对这个损失函数进行优化求最小化的极值，对应的一系列线性系数矩阵W,偏倚向量b即为我们的最终结果 